{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas openpyxl selenium seleniumbase beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from seleniumbase import Driver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.driver = None\n",
    "\n",
    "    def initialize_driver(self):\n",
    "        \"\"\"Initialize the web driver with undetected-chromedriver in headless mode.\"\"\"\n",
    "        try:\n",
    "            # Set up Chrome options for headless mode\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")  # Run headlessly\n",
    "            chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful in headless mode)\n",
    "            chrome_options.add_argument(\"--no-sandbox\")  # Sometimes required for headless environments (e.g., CI servers)\n",
    "\n",
    "            # Initialize the driver with the specified options\n",
    "            self.driver = Driver(uc=True, options=chrome_options)  # Pass chrome_options to the driver\n",
    "            self.driver.get(self.url)\n",
    "\n",
    "            logging.info(\"Driver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize driver: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def wait_for_element(self, by, value, timeout=10):\n",
    "        \"\"\"Wait for an element to be present on the page.\"\"\"\n",
    "        try:\n",
    "            element = WebDriverWait(self.driver, timeout).until(\n",
    "                EC.presence_of_element_located((by, value))\n",
    "            )\n",
    "            return element\n",
    "        except TimeoutException:\n",
    "            logging.warning(f\"Timeout waiting for element: {value}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_bill_data(self, year):\n",
    "        try:\n",
    "            select = Select(self.wait_for_element(By.ID, \"dnn_ctr17012_StateNetDB_ddlYear\"))\n",
    "            select.select_by_value(str(year))\n",
    "            search_button = self.wait_for_element(By.ID, \"dnn_ctr17012_StateNetDB_btnSearch\")\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", search_button)\n",
    "            self.driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, 'h2Headers1'))\n",
    "            )\n",
    "            time.sleep(5)  # Wait for the page to load completely\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            year_data = []\n",
    "            states = soup.find_all('div', class_='h2Headers1')\n",
    "            for state_element in states:\n",
    "                state = state_element.get_text(strip=True)\n",
    "                bill_divs = state_element.find_next_siblings('div')\n",
    "                for bill_div in bill_divs:\n",
    "                    if 'h2Headers1' in bill_div.get('class', []):\n",
    "                        break\n",
    "                    bill_data = self.extract_bill_info(bill_div, state, year)\n",
    "                    if bill_data:\n",
    "                        year_data.append(bill_data)\n",
    "            return year_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping bill data: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def get_field(self, elements, label):\n",
    "        try:\n",
    "            for element in elements:\n",
    "                if label in element.get_text(strip=True):\n",
    "                    next_sibling = element.next_sibling\n",
    "                    while next_sibling and (next_sibling.name == 'br' or not next_sibling.strip()):\n",
    "                        next_sibling = next_sibling.next_sibling\n",
    "                    if next_sibling and next_sibling.name is None:\n",
    "                        return next_sibling.strip()\n",
    "                    else:\n",
    "                        logging.warning(f\"Field '{label}' not found or has no valid sibling.\")\n",
    "                        return \"\"\n",
    "            logging.warning(f\"Field '{label}' not found.\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting field '{label}': {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_bill_info(self, bill_div, state, year):\n",
    "        try:\n",
    "            bill_link_element = bill_div.find('a')\n",
    "            bill_name = bill_link_element.get_text(strip=True) if bill_link_element else \"\"\n",
    "            bill_link = bill_link_element['href'] if bill_link_element else \"\"\n",
    "\n",
    "            bill_title_element = bill_div.find_next('div', style=\"font-weight: bold;\")\n",
    "            bill_title = bill_title_element.get_text(strip=True) if bill_title_element else \"\"\n",
    "\n",
    "            # Collect all <b> elements after the initial <div> and before the next <hr>\n",
    "            b_elements = []\n",
    "            next_sibling = bill_div.find_next_sibling()\n",
    "            while next_sibling and next_sibling.name != 'hr':\n",
    "                if next_sibling.name == 'b':\n",
    "                    b_elements.append(next_sibling)\n",
    "                next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "            status = self.get_field(b_elements, \"Status:\")\n",
    "            date_of_last_action = self.get_field(b_elements, \"Date of Last Action:\")\n",
    "\n",
    "            def get_authors():\n",
    "                primary_author = self.get_field(b_elements, \"Author:\")\n",
    "                additional_authors = self.get_field(b_elements, \"Additional Authors:\")\n",
    "                return f\"{primary_author}; {additional_authors}\".strip(\"; \")\n",
    "\n",
    "            authors = get_authors()\n",
    "            topics = self.get_field(b_elements, \"Topics:\")\n",
    "            summary = self.get_field(b_elements, \"Summary:\")\n",
    "\n",
    "            logging.info(f\"Extracted data for bill: {bill_name}\")\n",
    "            return {\n",
    "                'Year': year,\n",
    "                'State': state,\n",
    "                'Bill Name': bill_name,\n",
    "                'Bill Link': bill_link,\n",
    "                'Bill Title': bill_title,\n",
    "                'Status': status,\n",
    "                'Date of Last Action': date_of_last_action,\n",
    "                'Authors': authors,\n",
    "                'Topics': topics,\n",
    "                'Summary': summary\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting bill info: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_to_excel(self, data, year):\n",
    "        \"\"\"Save scraped data to Excel file.\"\"\"\n",
    "        try:\n",
    "            # Create a DataFrame from the scraped data\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Define the directory and filename\n",
    "            directory = \"../Data\"\n",
    "            filename = f'bills_data_{year}.xlsx'\n",
    "            \n",
    "            # Ensure the directory exists, create if not\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            # Full path to save the file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Save the DataFrame to the specified location\n",
    "            df.to_excel(file_path, index=False)\n",
    "\n",
    "            logging.info(f\"Data saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving data to Excel: {str(e)}\")\n",
    "\n",
    "    def run(self, years):\n",
    "        \"\"\"Run the scraper for specified years.\"\"\"\n",
    "        try:\n",
    "            self.initialize_driver()\n",
    "            for year in years:\n",
    "                logging.info(f\"Starting scrape for year {year}\")\n",
    "                year_data = self.scrape_bill_data(year)\n",
    "                self.save_to_excel(year_data, year)\n",
    "            self.driver.quit()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error running scraper: {str(e)}\")\n",
    "            if self.driver:\n",
    "                self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.ncsl.org/energy/energy-state-bill-tracking-archive-2008-2022\"\n",
    "    scraper = BillScraper(url)\n",
    "    scraper.run([2017, 2018])\n",
    "    # years = list(range(2008, 2023))  # 2008-2022\n",
    "    # scraper.run(years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
